#!/usr/bin/env python

# https://gallery.pangeo.io/repos/NCAR/cesm-lens-aws/notebooks/EnhancedIntakeCatalogDemo.html

##### Set up environment
# Done on test-env where intake-esm was installed via: 
#   conda create -n test-env -c conda-forge dask distributed ipython netcdf4 xarray intake-esm 
#   conda activate test-env

import intake
import pandas as pd
import pprint
# Not used here:
###import numpy as np
###import xarray as xr
# Allow multiple lines per cell to be displayed without print (default is just last line)
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
# Enable more explicit control of DataFrame display (e.g., to omit annoying line numbers)
from IPython.display import HTML

##### Inspect original intake-esm catalog
# Open original collection description file
cat_url_orig = 'https://ncar-cesm-lens.s3-us-west-2.amazonaws.com/catalogs/aws-cesm1-le.json'
coll_orig = intake.open_esm_datastore(cat_url_orig)

col_subset = coll_orig.search(frequency=["monthly"], component="ocn", variable="SST",
                        experiment=["20C", "RCP85"])
col_subset
col_subset.df

# Help from https://github.com/intake/intake-esm/issues/148
# Load catalog entries for subset into a dictionary of xarray datasets
dsets = col_subset.to_dataset_dict(zarr_kwargs={"consolidated": True}, storage_options={"anon": True})
print(f"\nDataset dictionary keys:\n {dsets.keys()}")

print(dsets)

# Define Xarray datasets corresponding to the three experiments
ds_20C = dsets['ocn.20C.monthly']
ds_RCP85 = dsets['ocn.RCP85.monthly']

# https://gallery.pangeo.io/repos/NCAR/cesm-lens-aws/notebooks/kay-et-al-2015.v3.html

##### Use Dask.Distributed utility function to display size of each dataset
from distributed.utils import format_bytes
print(f"20th Century: {format_bytes(ds_20C.nbytes)}\n"
      f"RCP8.5: {format_bytes(ds_RCP85.nbytes)}")

t_20c = ds_20C.SST
t_rcp = ds_RCP85.SST
t_20c

# The global surface temperature anomaly was computed relative to the 1961-90 base period
# in the Kay et al. paper, so extract that time slice #what should mine be? 
t_ref = t_20c.sel(time=slice("1961", "1990"))                          # ---------------- replace with Hillary's "DECOMPOSE OISST" code for anomalies

catalog_url = 'https://ncar-cesm-lens.s3-us-west-2.amazonaws.com/catalogs/aws-cesm1-le.json'
col = intake.open_esm_datastore(catalog_url)
col

cat = col.search(frequency="static", component="atm", experiment=["20C"])
_, grid = cat.to_dataset_dict(aggregate=False, zarr_kwargs={"consolidated": True}, storage_options={"anon": True}).popitem()
grid

cell_area = grid.area.load()
total_area = cell_area.sum()
cell_area

##### Define weighted means
# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects
t_20c_ts = ((t_20c.resample(time="AS").mean("time") * cell_area).sum(dim=("lat", "lon"))) / total_area #ANNUAL
t_rcp_ts = ((t_rcp.resample(time="AS").mean("time") * cell_area).sum(dim=("lat", "lon"))) / total_area #ANNUAL
print(t_rcp_ts)

print(t_20c.shape)
t_20c_ts = ((t_20c.resample(time="MS").mean("time") * cell_area).sum(dim=("lat", "lon"))) / total_area #MONTHLY
t_rcp_ts = ((t_rcp.resample(time="MS").mean("time") * cell_area).sum(dim=("lat", "lon"))) / total_area
print(t_20c_ts.shape); print(t_rcp_ts.shape)

t_20c_ts_new = t_20c_ts.isel(z_t=0)
t_rcp_ts_new = t_rcp_ts.isel(z_t=0)
print(t_20c_ts_new.shape); print(t_rcp_ts_new.shape)

ens1_20c = t_20c_ts_new[0,:1,:,:]
ens1_rcp = t_rcp_ts_new[0,:1,:,:]
print(ens1_20c.shape); print(ens1_rcp.shape)

%%time
ens1_20c_df = ens1_20c.to_series().unstack().T
ens1_20c_df.head()

# %%time
# t_20c_ts_df = t_20c_ts.to_series().unstack().T
# t_20c_ts_df.head()

# %%time
# t_rcp_ts_df = t_rcp_ts.to_series().unstack().T
# t_rcp_ts_df.head()

# ---------------------------------------------
# TASKS: use cell_area to regrid all of the
# CESMLE SST files so that it is on a normal 
# grid. Then, run Ocetrac on a that new gridded
# data. Load the files.
# ---------------------------------------------
